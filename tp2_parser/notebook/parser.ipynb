{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "import argparse\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGITS = re.compile(\"[0-9]\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the corpus\n",
    "file_path = os.path.join(os.getcwd(), 'sequoia-corpus+fct.mrg_strict')\n",
    "\n",
    "with open(file_path,'r') as f:\n",
    "    file = f.read()\n",
    "    sentences = file.split('\\n')[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "corpus_length = len(sentences)\n",
    "\n",
    "length_train = int(0.8 * corpus_length)\n",
    "length_dev = int(0.1 * corpus_length)\n",
    "length_test = int(0.1 * corpus_length)\n",
    "\n",
    "end_dev = length_train + length_dev\n",
    " \n",
    "\n",
    "corpus_train = sentences[:length_train]\n",
    "corpus_dev = sentences[length_train:end_dev]\n",
    "corpus_test = sentences[end_dev:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file with non-parsed sentences\n",
    "def unparse(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    \n",
    "    list_sentence = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        if ')' in token:\n",
    "            word = ''\n",
    "            for caract in token:\n",
    "                if caract == ')':\n",
    "                    break\n",
    "                else:\n",
    "                    word += caract\n",
    "            list_sentence.append(word)\n",
    "            \n",
    "    sent = listToString(list_sentence)\n",
    "    \n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert list to string\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for idx, ele in enumerate(s):\n",
    "        if idx == len(s)-1:\n",
    "            str1 += ele\n",
    "        else:\n",
    "            str1 += ele + ' '  \n",
    "    \n",
    "    # return string   \n",
    "    return str1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove functional tags\n",
    "def functional_tags_out(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    new_sentence = []\n",
    "    for part_sent in sentence:\n",
    "        if '(' in part_sent:\n",
    "            new_part_sent = ''\n",
    "            for caract in part_sent:\n",
    "                if caract == '-':\n",
    "                    break\n",
    "                new_part_sent += caract\n",
    "        else:\n",
    "            new_part_sent = part_sent\n",
    "            \n",
    "        new_sentence.append(new_part_sent)\n",
    "    \n",
    "    return listToString(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rule to dictionary\n",
    "def add_dic(dico, tag, rules, count):\n",
    "    if tag not in dico.keys():\n",
    "        dico[tag] = {}\n",
    "    if rules not in dico[tag]:\n",
    "        dico[tag][rules] = count\n",
    "    else:\n",
    "        dico[tag][rules] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize counts\n",
    "def count_to_freq(dico):\n",
    "    for tag in dico:\n",
    "            total = 0\n",
    "            for rule in dico[tag].keys():\n",
    "                total += dico[tag][rule]\n",
    "            \n",
    "            \n",
    "            for rule in dico[tag].keys():\n",
    "                dico[tag][rule] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tags from grammar\n",
    "def get_tags(dico):\n",
    "    tags_list = []\n",
    "    for tag in dico:\n",
    "        if tag not in tags_list:\n",
    "            tags_list.append(tag)\n",
    "            \n",
    "        for rule in dico[tag].keys():\n",
    "            list_child_tags = rule.split(' ')\n",
    "            for child_tag in list_child_tags:\n",
    "                if child_tag not in tags_list:\n",
    "                    tags_list.append(child_tag)\n",
    "                    \n",
    "    return tags_list                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the word is not available in the vocabulary,\n",
    "# we can try multiple case normalizing procedure.\n",
    "# We consider the best substitute to be the one with the lowest index,\n",
    "# which is equivalent to the most frequent alternative.\n",
    "def case_normalizer(word, dictionary):\n",
    "    w = word\n",
    "    lower = (dictionary.get(w.lower(), 1e12), w.lower())\n",
    "    upper = (dictionary.get(w.upper(), 1e12), w.upper())\n",
    "    title = (dictionary.get(w.title(), 1e12), w.title())\n",
    "    results = [lower, upper, title]\n",
    "    results.sort()\n",
    "    index, w = results[0]\n",
    "    if index != 1e12:\n",
    "        return w\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the word is OOV, find the closest alternative\n",
    "def normalize(word, word_id):\n",
    "    if word not in word_id:\n",
    "        word = DIGITS.sub(\"#\", word)\n",
    "    \n",
    "    if word not in word_id:\n",
    "        word = case_normalizer(word, word_id)\n",
    "\n",
    "    if word not in word_id:\n",
    "        return None\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts words according to their Euclidean distance.\n",
    "# To use cosine distance, embeddings has to be normalized so that their l2 norm is 1.\n",
    "# indeed (a-b)^2\"= a^2 + b^2 - 2a^b = 2*(1-cos(a,b)) of a and b are norm 1\"\"\"\n",
    "def l2_nearest(embeddings, query_embedding, k):\n",
    "    distances = (((embeddings - query_embedding) ** 2).sum(axis=1) ** 0.5)\n",
    "    sorted_distances = sorted(enumerate(distances), key=itemgetter(1))\n",
    "    return zip(*sorted_distances[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to parsed sentence\n",
    "def list_to_parsed_sentence(parsing):\n",
    "        if type(parsing) == str:\n",
    "            return parsing\n",
    "\n",
    "        else:\n",
    "            string = \"\"\n",
    "            for p in parsing:\n",
    "                root_tag = p[0]\n",
    "                parsing_substring = p[1]\n",
    "                string = string + \"(\" + root_tag + \" \" + list_to_parsed_sentence(parsing_substring) + \")\" + \" \"\n",
    "            string = string[:-1]  # Remove the extra space\n",
    "            return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a Tree from a tagged sentence\n",
    "def tagged_sent_to_tree(tagged_sent, remove_after_hyphen=True):\n",
    "    max_id_node = 0\n",
    "\n",
    "    tree = nx.DiGraph()\n",
    "\n",
    "    sent = tagged_sent.split()\n",
    "    hierarchy = list()\n",
    "\n",
    "    hierarchy.append([])\n",
    "\n",
    "    level = 0  # difference between the number of opened and closed parenthesis\n",
    "\n",
    "    for (idx_bloc, bloc) in enumerate(sent):\n",
    "\n",
    "        if bloc[0] == \"(\":\n",
    "\n",
    "            if remove_after_hyphen:\n",
    "                tag = clean_tag(bloc[1:])  # we add it to the hierarchy\n",
    "            else:\n",
    "                tag = bloc[1:]\n",
    "            if level < len(hierarchy):  # there is already one tag as its level\n",
    "                hierarchy[level].append((tag, max_id_node))\n",
    "            else:  # first tag as its level\n",
    "                hierarchy.append([(tag, max_id_node)])\n",
    "            if idx_bloc > 0:\n",
    "                tree.add_node(max_id_node, name=tag)\n",
    "                max_id_node += 1\n",
    "            level += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            word = \"\"\n",
    "            nb_closing_brackets = 0\n",
    "            for caract in bloc:\n",
    "                if caract == \")\":\n",
    "                    nb_closing_brackets += 1\n",
    "                else:\n",
    "                    word += caract\n",
    "\n",
    "            tree.add_node(max_id_node, name=word)\n",
    "            tree.add_edge(max_id_node - 1, max_id_node)\n",
    "            max_id_node += 1\n",
    "\n",
    "            level -= nb_closing_brackets\n",
    "\n",
    "            for k in range(nb_closing_brackets - 1, 0, -1):\n",
    "                root = hierarchy[-2][-1][0]  # root tag\n",
    "                id_root = hierarchy[-2][-1][1]\n",
    "                if root == '':\n",
    "                    break\n",
    "                tags = hierarchy[-1]  # child tags\n",
    "\n",
    "                for tag in tags:\n",
    "                    tree.add_edge(id_root, tag[1])\n",
    "\n",
    "                hierarchy.pop()\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial sentence from subtree rooted at node\n",
    "def tree_to_sentence_helper(tree, node):\n",
    "    children = list(tree.successors(node))\n",
    "    if (len(children) == 1) and (len(list(tree.successors(children[0]))) == 0):\n",
    "        return \"(\" + tree.nodes[node][\"name\"] + \" \" + tree.nodes[children[0]][\"name\"] + \")\"\n",
    "    else:\n",
    "        res = \"(\" + tree.nodes[node][\"name\"]\n",
    "        for child in sorted(children):\n",
    "            res += \" \" + tree_to_sentence_helper(tree, child)\n",
    "        res += \")\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree to sentence\"\"\"\n",
    "def tree_to_sentence(tree):\n",
    "    root = list(nx.topological_sort(tree))[0]\n",
    "    return \"( \" + tree_to_sentence_helper(tree, root) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCFG Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent, value):\n",
    "        self.parent = parent\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "\n",
    "    def GetChildrenValues(self):\n",
    "        return list(map(lambda x: x.value, self.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCFG_Tree:\n",
    "    def __init__(self, sentence):\n",
    "        self.root = Node(None, 'SENT')\n",
    "        self.grammar = {'SENT' : {}}\n",
    "        self.lexicon = {}\n",
    "        self.inv_lexicon = {}\n",
    "        \n",
    "        sentence = functional_tags_out(sentence)\n",
    "        current_node = self.root\n",
    "        sentence = sentence.split(' ')\n",
    "        sentence = sentence[2:]\n",
    "        \n",
    "        \n",
    "        for idx, word in enumerate(sentence):\n",
    "            if '(' in word:\n",
    "                current_tag = word.replace('(', '')\n",
    "                new_node = Node(current_node, current_tag)\n",
    "                current_node.children.append(new_node)\n",
    "                current_node = new_node\n",
    "                \n",
    "                if current_tag not in self.grammar:\n",
    "                    self.grammar[current_tag] = {}\n",
    "                    \n",
    "            else:\n",
    "                num_closed_brackets = 0\n",
    "                ele = ''\n",
    "                for caract in word:\n",
    "                    if caract == ')':\n",
    "                        num_closed_brackets += 1\n",
    "                    else:\n",
    "                        ele += caract\n",
    "                        \n",
    "                \n",
    "                add_dic(self.inv_lexicon, ele, current_tag, 1)\n",
    "                        \n",
    "                add_dic(self.lexicon, current_tag, ele, 1)\n",
    "                \n",
    "                for i in range(num_closed_brackets):\n",
    "                    if current_node.parent is not None:\n",
    "                        current_node = current_node.parent\n",
    "    \n",
    "    def ExtractGrammar(self, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if len(node.children) > 0:\n",
    "            rule = listToString(node.GetChildrenValues())\n",
    "            if rule not in self.grammar[node.value]:\n",
    "                self.grammar[node.value][rule] = 1\n",
    "            else:\n",
    "                self.grammar[node.value][rule] += 1\n",
    "            \n",
    "            for child in node.children:\n",
    "                self.ExtractGrammar(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to build the PCFG in Chomsky normal form\n",
    "class PCFG:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "\n",
    "        # Initialize grammar, lexicon and new tags\n",
    "        self.grammar = {}\n",
    "        self.lexicon = {}\n",
    "        self.inv_lexicon = {}\n",
    "        self.set_new_tags = set()\n",
    "\n",
    "        # Build PCFG from corpus\n",
    "        self.build_pcfg(corpus)\n",
    "\n",
    "        # Build dictionnary of words form the corpus with the corresponding frequency \n",
    "        # {word : frequency}\n",
    "        self.tokens = {}\n",
    "        for tag in self.lexicon.keys():\n",
    "            for word in self.lexicon[tag].keys():\n",
    "                if word in self.tokens.keys():\n",
    "                    self.tokens[word] += self.lexicon[tag][word]\n",
    "                else:\n",
    "                    self.tokens[word] = self.lexicon[tag][word]\n",
    "        sum = np.sum(list(self.tokens.values()))\n",
    "        \n",
    "        for word in self.tokens:\n",
    "            self.tokens[word] /= sum\n",
    "\n",
    "            \n",
    "        # Apply Chomsky normalization to PCFG from corpus\n",
    "        self.to_chomsky_form()\n",
    "\n",
    "        \n",
    "        # Build dictionnary of terminal tags form the corpus with the corresponding frequency \n",
    "        # {terminal_tag : frequency}\n",
    "        self.terminal_tags = {tag: np.sum(list(counts.values())) for (tag, counts) in self.lexicon.items()}\n",
    "        sum = np.sum(list(self.terminal_tags.values()))\n",
    "        for tag in self.terminal_tags:\n",
    "            self.terminal_tags[tag] /= sum\n",
    "\n",
    "        \n",
    "        # Normalize\n",
    "        count_to_freq(self.grammar)\n",
    "        count_to_freq(self.lexicon)\n",
    "        count_to_freq(self.inv_lexicon)\n",
    "\n",
    "        \n",
    "        # Get all tags\n",
    "        list_all_tags = get_tags(self.grammar)\n",
    "        \n",
    "        # Tags lists\n",
    "        self.list_new_tags = list(self.set_new_tags)\n",
    "        self.list_tags = list(set(list_all_tags).difference(self.set_new_tags))\n",
    "        self.list_all_tags = self.list_tags + self.list_new_tags\n",
    "        self.dic_all_tags = {word: idx for (idx, word) in enumerate(self.list_all_tags)}\n",
    "        \n",
    "        self.nb_tags = len(self.list_tags)\n",
    "        self.nb_all_tags = len(self.list_all_tags)\n",
    "\n",
    "        \n",
    "    # Apply Chomsky normalization to PCFG\n",
    "    def to_chomsky_form(self):\n",
    "        self.to_binary_rules()\n",
    "        self.unit_rules_out()\n",
    "\n",
    "        \n",
    "    # Telescope unait rules\n",
    "    def unit_rules_out(self):\n",
    "        grammar_copy = deepcopy(self.grammar)\n",
    "        lexicon_copy = deepcopy(self.lexicon)\n",
    "\n",
    "        rules_to_remove = []\n",
    "\n",
    "        for tag in grammar_copy.keys():\n",
    "            \n",
    "            for rule in grammar_copy[tag].keys():\n",
    "                count = grammar_copy[tag][rule]\n",
    "                list_tags = rule.split(' ')\n",
    "                \n",
    "                if len(list_tags) == 1: # Check if unit rule\n",
    "\n",
    "                    child_tag = list_tags[0]\n",
    "                    rules_to_remove.append((tag, child_tag))\n",
    "                    proba = count / (np.sum(list(self.grammar[tag].values())))\n",
    "\n",
    "                    # rule A -> B where B is a pre-terminal tag\n",
    "                    if child_tag in lexicon_copy:\n",
    "                        \n",
    "                        if tag != \"SENT\":\n",
    "                            new_tag = tag + \"&\" + child_tag\n",
    "                            self.set_new_tags.add(new_tag)\n",
    "                            \n",
    "                            # new_tag -> word\n",
    "                            for word in lexicon_copy[child_tag].keys():\n",
    "                                count2 = lexicon_copy[child_tag][word]\n",
    "                                add_dic(self.lexicon, new_tag, word, count2*proba)\n",
    "\n",
    "                            \n",
    "                            # for each rule X -> Y A, add rule X -> Y A&B\n",
    "                            for tag2 in grammar_copy.keys():\n",
    "                                for rule2 in grammar_copy[tag2].keys():\n",
    "                                    list_tags = rule2.split(' ')\n",
    "                                    count2 = grammar_copy[tag2][rule2]\n",
    "                                    if len(list_tags) == 2 and list_tags[1] == tag:\n",
    "                                        new_rule2 = rule2.replace(tag, new_tag)\n",
    "                                        add_dic(self.grammar, tag2, new_rule2, count2)\n",
    "\n",
    "                    # If B not pre-terminal, for each rule B -> X Y, add A -> X Y\n",
    "                    else:\n",
    "                    \n",
    "                        for grand_child_tag in grammar_copy[child_tag].keys():\n",
    "                            list_grand_child_tags = grand_child_tag.split(' ')\n",
    "                            count3 = grammar_copy[child_tag][grand_child_tag]\n",
    "                            if len(list_grand_child_tags) == 2:\n",
    "                                add_dic(self.grammar, tag, grand_child_tag, count3*proba)\n",
    "                \n",
    "\n",
    "        for (left, right) in rules_to_remove:\n",
    "            del self.grammar[left][right]\n",
    "            \n",
    "        for tag in grammar_copy.keys():\n",
    "            if len(self.grammar[tag]) == 0:\n",
    "                del self.grammar[tag]\n",
    "\n",
    "    \n",
    "    # Replace all rules with more than 2 children with a chain of rule\n",
    "    def to_binary_rules(self):\n",
    "        grammar_copy = deepcopy(self.grammar)\n",
    "    \n",
    "    \n",
    "        for tag in grammar_copy.keys():\n",
    "        \n",
    "            for rule in grammar_copy[tag].keys():\n",
    "            \n",
    "                count = grammar_copy[tag][rule]\n",
    "                tags_list = rule.split(' ')\n",
    "            \n",
    "                if len(tags_list) > 2: \n",
    "                    del self.grammar[tag][rule]\n",
    "                    old_tag = tag\n",
    "                    for idx, sub_tag in enumerate(tags_list[:-2]):\n",
    "                        new_symbol = tag + '|' + '-'.join(tags_list[idx+1:])\n",
    "                        self.set_new_tags.add(new_symbol)\n",
    "                        new_rule = listToString([sub_tag, new_symbol])\n",
    "                        add_dic(self.grammar, old_tag, new_rule, count)\n",
    "                        old_tag = new_symbol\n",
    "                    \n",
    "                    last_two_tags = listToString(tags_list[-2:])\n",
    "                    add_dic(self.grammar, old_tag, last_two_tags, count)\n",
    "                                                     \n",
    " \n",
    "                    \n",
    "    # Build the PCFG\n",
    "    def build_pcfg(self, corpus):\n",
    " \n",
    "        for sentence in corpus:\n",
    "            tree = PCFG_Tree(sentence)\n",
    "            tree.ExtractGrammar()\n",
    "        \n",
    "            lexicon_sent = tree.lexicon\n",
    "            inv_lexicon_sent = tree.inv_lexicon\n",
    "            grammar_sent = tree.grammar\n",
    "            \n",
    "        \n",
    "            # Add lexicon of sentence in general lexicon\n",
    "            for tag in lexicon_sent.keys():\n",
    "                for word in lexicon_sent[tag].keys():\n",
    "                    add_dic(self.lexicon, tag, word, 1)\n",
    "                    \n",
    "            # Add inverse lexicon of sentence in general inverse lexicon\n",
    "            for word in inv_lexicon_sent.keys():\n",
    "                for tag in inv_lexicon_sent[word]:\n",
    "                    add_dic(self.inv_lexicon, word, tag, 1)\n",
    "                \n",
    "            # Add grammar of sentence in general grammar      \n",
    "            for tag in grammar_sent.keys():\n",
    "                for rule in grammar_sent[tag].keys():\n",
    "                    add_dic(self.grammar, tag, rule, 1)\n",
    "                \n",
    "            \n",
    "        # Get rid of terminal tags\n",
    "        self.grammar = { tag : dic for tag,dic in self.grammar.items() if len(dic) > 0}             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to handle OOV words\n",
    "class OOV:\n",
    "    \n",
    "    def __init__(self, lexicon, list_all_tags, tokens):\n",
    "        \n",
    "        # Lexicon from PCFG\n",
    "        self.lexicon = lexicon\n",
    "        self.tokens = tokens\n",
    "        \n",
    "        # Words in the Polyglott embeddings\n",
    "        self.poly_words, self.poly_embeddings = pickle.load(open('data/polyglot-fr.pkl', \"rb\"), \n",
    "                                                           encoding='bytes')\n",
    "        \n",
    "        # List of words in the lexicon                                    \n",
    "        self.words_lexicon = list(tokens.keys())\n",
    "        self.words_lexicon_id = {word: idx for (idx, word) in enumerate(self.words_lexicon)}\n",
    "        \n",
    "        # Assign Id's to each word in Polyglott embeddings\n",
    "        self.poly_words_id = {word: idx for (idx, word) in enumerate(self.poly_words)}\n",
    "        \n",
    "        # Embedding matrix\n",
    "        self.lexicon_embeddings = None\n",
    "        \n",
    "        # Words in the lexicon having a polyglott embedding\n",
    "        self.lexicon_words_with_embed = []\n",
    "        self.lexicon_words_with_embed_id = {}\n",
    "        \n",
    "        # Build embedding matrix with words from lexion\n",
    "        self.build_embeddings_lexicon()\n",
    "        self.lexicon_embeddings /= np.linalg.norm(self.lexicon_embeddings, axis=1)[:, None]  # normalize embeddings\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    # Returns the closest word using a combination of levi distance and cosine similarity\n",
    "    # of embeddings (if available)\n",
    "    def closest_word(self, query):\n",
    "            \n",
    "        query_normalized = normalize(query, self.words_lexicon_id)\n",
    "            \n",
    "        # query_normalized exists in lexicon\n",
    "        if query_normalized is not None:\n",
    "            return query_normalized\n",
    "            \n",
    "        # Embedding of query exists in Polyglott embeddings, then return word\n",
    "        # with closest embedding\n",
    "        if query in self.poly_words:\n",
    "            closest_word = self.closest_word_embedding(query)\n",
    "            return closest_word\n",
    "            \n",
    "        # If embedding of query does not exists in in Polyglott embeddings, then return\n",
    "        # word with closest levi distance and highest frequency\n",
    "        else:\n",
    "            closest_word = self.closest_word_levi(query,2)\n",
    "            return closest_word\n",
    "            \n",
    "    \n",
    "    # Build an emebdding matrix with words from the lexicon having on \n",
    "    # in the Polyglott embeddings\n",
    "    def build_embeddings_lexicon(self):\n",
    "        \n",
    "        # Get embedding of the word if in the polyglott embeddings\n",
    "        for word in self.words_lexicon:\n",
    "            word_normalized = normalize(word, self.poly_words_id)\n",
    "                \n",
    "            if word_normalized is not None:\n",
    "                self.lexicon_words_with_embed.append(word)\n",
    "                id_word = self.poly_words_id[word_normalized]\n",
    "                    \n",
    "                if self.lexicon_embeddings is None:\n",
    "                    self.lexicon_embeddings = self.poly_embeddings[id_word]\n",
    "                else:\n",
    "                    self.lexicon_embeddings = np.vstack([self.lexicon_embeddings, self.poly_embeddings[id_word]])\n",
    "\n",
    "            \n",
    "        # Assign new indexes to words having a polyglott embedding\n",
    "        self.lexicon_words_with_embed_id = {w: i for (i, w) in enumerate(self.lexicon_words_with_embed)}\n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "    # Returns word with closest embedding to query\n",
    "    def closest_word_embedding(self, query):\n",
    "            \n",
    "            \n",
    "        # Check if query has polyglott embedding\n",
    "        query = normalize(query, self.poly_words_id)\n",
    "            \n",
    "        if query is None:\n",
    "            print('Query has no embedding in the Polyglott embeddings')\n",
    "            return None\n",
    "            \n",
    "            \n",
    "        # If query has polyglott embedding, returns word in the lexicon\n",
    "        # with closest embedding\n",
    "        query_idx = self.poly_words_id[query]\n",
    "        query_embedding = self.poly_embeddings[query_idx]\n",
    "        query_embedding /= np.linalg.norm(query_embedding) # normalize\n",
    "        indices, distances = l2_nearest(self.lexicon_embeddings, query_embedding, 1)\n",
    "            \n",
    "        idx = indices[0]\n",
    "            \n",
    "        closest_word = self.lexicon_words_with_embed[idx]\n",
    "            \n",
    "        return closest_word\n",
    "        \n",
    "    # Returns Levenshtein distance between two strings\n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        # Convert string to list\n",
    "        list1 = list(s1)\n",
    "        list2 = list(s2)\n",
    "        N1 = len(list1)\n",
    "        N2 = len(list2)\n",
    "    \n",
    "        # Initialize  Matrix\n",
    "        mat = np.zeros((N1+1,N2+1))\n",
    "        mat[:,0] = np.arange(N1+1)\n",
    "        mat[0,:] = np.arange(N2+1)\n",
    "\n",
    "        # Dynamic Programming\n",
    "        for i in range(1, N1+1):\n",
    "            for j in range(1, N2+1):\n",
    "                if list1[i-1] == list2[j-1]:\n",
    "                    mat[i, j] = min(mat[i-1, j]+1, mat[i,j-1]+1, mat[i-1,j-1])\n",
    "                else:\n",
    "                    mat[i, j] = min(mat[i-1,j]+1, mat[i, j-1]+1, mat[i-1,j-1]+1)\n",
    "                \n",
    "        return mat[-1,-1] \n",
    "\n",
    "        \n",
    "        \n",
    "    def closest_word_levi(self, query, k):\n",
    "            \n",
    "        candidates = {}\n",
    "            \n",
    "        # Separate candidates with different levi distance\n",
    "        for i in range(1,k+1):\n",
    "            candidates[i] = [] \n",
    "                \n",
    "    \n",
    "        min_dist = k\n",
    "            \n",
    "        # Find candidates\n",
    "        for word in self.words_lexicon:\n",
    "            levi_dist = self.levenshtein_distance(word, query)\n",
    "            if levi_dist <= min_dist:\n",
    "                candidates[levi_dist].append(word)\n",
    "                min_dist = levi_dist\n",
    "            \n",
    "            \n",
    "        # Find final candidates (with the lowest levi distance)\n",
    "        final_candidates = None\n",
    "        for i in range(1,k+1):\n",
    "            if len(candidates[i]) > 0:\n",
    "                final_candidates = candidates[i]\n",
    "                break\n",
    "            \n",
    "        if final_candidates == None:\n",
    "            print('No words within a levenshtein distance of {} for: {}'.format(k,query))\n",
    "            return None\n",
    "                    \n",
    "                \n",
    "        # Get frequency of each candidate\n",
    "        freq_final_candidates = []\n",
    "        for final_candidate in final_candidates:\n",
    "            freq_final_candidates.append(self.tokens[final_candidate])\n",
    "                \n",
    "                \n",
    "        idx_max_freq = np.argmax(freq_final_candidates)\n",
    "            \n",
    "        return final_candidates[idx_max_freq]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CYK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to apply CYK algorithm\n",
    "class CYK:\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        \n",
    "        # PCFG and OOV class\n",
    "        self.pcfg = PCFG(corpus)\n",
    "        self.oov = OOV(self.pcfg.lexicon, self.pcfg.list_all_tags, self.pcfg.tokens)\n",
    "        \n",
    "        # Initialize CYP probability matrix\n",
    "        self.proba_matrix = None\n",
    "        self.cyk_matrix = None\n",
    "              \n",
    "        \n",
    "    # Apply the CYK algorithm\n",
    "    def CYK_algorithm(self, sentence):\n",
    "        \n",
    "        # Initialize\n",
    "        n = len(sentence)\n",
    "        r = self.pcfg.nb_all_tags\n",
    "        P = np.zeros((n,n,r))\n",
    "        cyk_matrix = np.zeros((n,n,r,3))\n",
    "    \n",
    "    \n",
    "        # First level P[0,:,:]\n",
    "        for idx_word, word in enumerate(sentence):\n",
    "            \n",
    "            # Get closest word in the lexicon\n",
    "            word = self.oov.closest_word(word)\n",
    "            \n",
    "            if word is None:\n",
    "                for idx_tag, tag in enumerate(self.pcfg.list_all_tags):\n",
    "                    if tag in self.pcfg.terminal_tags:\n",
    "                        P[0, idx_word, idx_tag] = self.pcfg.terminal_tags[tag]\n",
    "                        \n",
    "            else:\n",
    "                for idx_tag, tag in enumerate(self.pcfg.list_all_tags):\n",
    "                    if tag in self.pcfg.inv_lexicon[word]:\n",
    "                        P[0, idx_word, idx_tag] = self.pcfg.inv_lexicon[word][tag]\n",
    "                   \n",
    "                        \n",
    "                        \n",
    "        # Other levels\n",
    "        for l in range(1, n):\n",
    "        \n",
    "            for s in range(n-l):\n",
    "            \n",
    "                for tag in self.pcfg.grammar:\n",
    "                    idx_tag = self.pcfg.dic_all_tags[tag]\n",
    "                \n",
    "                    for p in range(l):\n",
    "                    \n",
    "                        for rule in self.pcfg.grammar[tag]:\n",
    "                            left_tag = rule.split(' ')[0]\n",
    "                            right_tag = rule.split(' ')[1]\n",
    "                            b = self.pcfg.dic_all_tags[left_tag]\n",
    "                            c = self.pcfg.dic_all_tags[right_tag]\n",
    "                    \n",
    "                            prob_splitting = self.pcfg.grammar[tag][rule] * P[p, s, b] * P[l-p-1, s+p+1, c]\n",
    "                        \n",
    "                            if prob_splitting > P[l, s, idx_tag]:\n",
    "                                P[l, s, idx_tag] = prob_splitting\n",
    "                                cyk_matrix[l, s, idx_tag] = [p, b, c]\n",
    "                        \n",
    "                \n",
    "                \n",
    "                \n",
    "        self.proba_matrix = P\n",
    "        self.cyk_matrix = cyk_matrix.astype(int)\n",
    "        \n",
    "    # Remove new tags and de-telescope tags\n",
    "    def clean_tags(self, tree):\n",
    "        # remove new tags of type\n",
    "        nodes = deepcopy(tree.nodes)\n",
    "        for node in nodes:\n",
    "            children = list(tree.successors(node))\n",
    "            \n",
    "            if len(children) == 0:\n",
    "                pass\n",
    "            \n",
    "            elif len(children) == 1 and len(list(tree.successors(children[0]))) == 0:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                parent = list(tree.predecessors(node))\n",
    "                if len(parent) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    tag = tree.nodes[node][\"name\"]\n",
    "                    \n",
    "                    if (self.pcfg.dic_all_tags[tag] >= self.pcfg.nb_tags) and (\n",
    "                            \"|\" in tag):  \n",
    "                        \n",
    "                        for child in tree.successors(node):\n",
    "                            tree.add_edge(parent[0], child)\n",
    "                        tree.remove_node(node)\n",
    "\n",
    "        # Decomposing A&B -> w into A -> B -> w\n",
    "        max_node = np.max(tree.nodes())\n",
    "        nodes = deepcopy(tree.nodes)\n",
    "        for node in nodes:\n",
    "            \n",
    "            children = list(tree.successors(node))\n",
    "            \n",
    "            if len(children) == 0 or len(list(tree.predecessors(node))) == 0:\n",
    "                pass\n",
    "            \n",
    "            elif len(children) == 1 and len(list(tree.successors(children[0]))) == 0:\n",
    "                tag = tree.nodes[node][\"name\"]\n",
    "\n",
    "                if (self.pcfg.dic_all_tags[tag] >= self.pcfg.nb_tags) and (\n",
    "                        \"&\" in tag):  # new tag from unit rule\n",
    "                    word = children[0]\n",
    "\n",
    "                    idx_cut = None\n",
    "                    \n",
    "                    for (idx, c) in enumerate(tag):\n",
    "                        if c == \"&\":\n",
    "                            idx_cut = idx\n",
    "\n",
    "                    tree.nodes[node][\"name\"] = tag[:idx_cut]\n",
    "\n",
    "                    idx_pre_terminal_node = max_node + 1\n",
    "                    tree.add_node(idx_pre_terminal_node, name=tag[idx_cut + 1:])\n",
    "                    max_id_node += 1\n",
    "                    tree.remove_edge(node, word)\n",
    "                    tree.add_edge(node, idx_pre_terminal_node)\n",
    "                    tree.add_edge(idx_pre_terminal_node, word)\n",
    "    \n",
    "    \n",
    "    # Parse part of a sentence\n",
    "    def parse_substring(self, s, l, idx_tag, sentence):\n",
    "    \n",
    "\n",
    "        if l == 0:\n",
    "            return sentence[s]\n",
    "\n",
    "        else: \n",
    "            cut = self.cyk_matrix[l, s, idx_tag, 0]\n",
    "            idx_left_tag = self.cyk_matrix[l, s, idx_tag, 1]\n",
    "            idx_right_tag = self.cyk_matrix[l, s, idx_tag, 2]\n",
    "\n",
    "            left_tag = self.pcfg.list_all_tags[idx_left_tag]\n",
    "            right_tag = self.pcfg.list_all_tags[idx_right_tag]\n",
    "\n",
    "            return [[left_tag, self.parse_substring(s, cut, idx_left_tag, sentence)],\n",
    "                    [right_tag, self.parse_substring(s + cut + 1, l - cut - 1, idx_right_tag, sentence)]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Returns the parsed sentence\n",
    "    def parse(self, sentence):\n",
    "        \n",
    "        sentence = sentence.split(' ')\n",
    "        length_sentence = len(sentence)\n",
    "        \n",
    "        \n",
    "        if length_sentence > 1:\n",
    "            self.CYK_algorithm(sentence)\n",
    "            idx_root_tag = self.pcfg.dic_all_tags['SENT']\n",
    "            if self.proba_matrix[length_sentence - 1][0][idx_root_tag] == 0:  # no valid parsing\n",
    "                return None\n",
    "            parsing_list = self.parse_substring(0, length_sentence - 1, idx_root_tag, sentence)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            word = sentence[0]\n",
    "            word_lexicon = self.oov.closest_word(word)\n",
    "            \n",
    "            \n",
    "            if word_lexicon is None:\n",
    "                tag = max(self.pcfg.terminal_tags, key=self.pcfg.terminal_tags.get)\n",
    "                \n",
    "            else:\n",
    "                tag = max(self.pcfg.inv_lexicon[word_lexicon], key=self.pcfg.inv_lexicon[word_lexicon].get)\n",
    "            \n",
    "            parsing_list = '(' + tag + word + ')'\n",
    "            \n",
    "        # converting the parsing stored as a string into a tree\n",
    "        tree = tagged_sent_to_tree(\"( (SENT \" + list_to_parsed_sentence(parsing_list) + \"))\",\n",
    "                                   remove_after_hyphen=False)\n",
    "            \n",
    "        self.clean_tags(tree)\n",
    "            \n",
    "            \n",
    "        return tree_to_sentence(tree)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_corpus.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "    test_sentences = file.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the parser with corpus_train\n",
    "print('Building the parser...')\n",
    "cyk_parser = CYK(corpus_train)\n",
    "print('Done')\n",
    "\n",
    "# Begin parsing\n",
    "print('Parsing...')\n",
    "\n",
    "with open('evaluation_data.parser.txt', 'w') as f:\n",
    "    for sentence in test_sentences:\n",
    "        print('Sentence: ' + sentence + '\\n')\n",
    "        parsed_sentence = cyk_parser.parse(sentence)\n",
    "        if parsed_sentence is not None:\n",
    "            print('Parsed sentence: ' + parsed_sentence + '\\n')\n",
    "            f.write('%s\\n' % parsed_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
